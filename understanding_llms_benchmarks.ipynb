{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets==2.18.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golongson/miniconda3/envs/eval/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding LLMs Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLMs are becoming so popular that it's really difficult to keep up with all the new releases, new variants, fine-tuning, merges, and so on. In this notebook, we will delve step by step into understanding how LLMs are evaluated today and try to grasp the various aspects in detail.\n",
    "\n",
    "The following image shows a current snapshot of the OpenLLM Leaderboard available at the following URL (https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n",
    "\n",
    "![openllm_leaderboard.PNG](images/1_1_openllm_leaderboard.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submitted models are evaluated on 6 main benchmarks:\n",
    "\n",
    "- ARC\n",
    "- HellaSwag\n",
    "- MMLU\n",
    "- TruthfulQA\n",
    "- Winogrande\n",
    "- GSM8K\n",
    "\n",
    "#### Let's try to analyze them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARC\n",
    "\n",
    "ARC stands for AI2 Reasoning Challenge. It's a dataset released by the Allen Institute in 2018 along with the paper, which can be viewed at the following URL (https://arxiv.org/pdf/1803.05457.pdf). It's a question-answering dataset designed to evaluate a model's knowledge and reasoning abilities. The dataset consists of 7787 multiple-choice questions with a wide range of difficulty levels. The questions are divided into \"easy\" and \"challenge\" sets, testing different levels of knowledge such as definitions, objectives, processes, and algebra. It was designed to be a more complex version of the famous SQuAD (Stanford Question Answering Dataset).\n",
    "\n",
    "Unlike SQuAD, which evaluates the ability to extract the answer from a provided passage. In SQuAD usually, all the information needed to answer a certain question is contained within the dataset, but in different points. \n",
    "ARC \n",
    "\n",
    "ARC does not test the model's extraction ability but rather its capacity to leverage its internal knowledge and reasoning to provide the correct answer. Clearly, since the answers are provided in multiple-choice format, the ability to correlate objects to obtain the correct response is also evaluated.\n",
    "\n",
    "One issue is that all the questions are of a scientific nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2d6b760e294063812fd59955347083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|███████████████████████████████████████████████████████████████| 331k/331k [00:01<00:00, 221kB/s]\n",
      "Downloading data: 100%|██████████████████████████████████████████████████████████████| 346k/346k [00:00<00:00, 3.41MB/s]\n",
      "Downloading data: 100%|█████████████████████████████████████████████████████████████| 86.1k/86.1k [00:00<00:00, 945kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d8978f7b894bfa994aa59ef82c5aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd5a8d9d7714760a5aaeaad7ee80bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2376 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4dd681687444fa98c8753284fa6dbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████████████████████████████████████████████████████████| 190k/190k [00:00<00:00, 2.02MB/s]\n",
      "Downloading data: 100%|██████████████████████████████████████████████████████████████| 204k/204k [00:00<00:00, 2.43MB/s]\n",
      "Downloading data: 100%|█████████████████████████████████████████████████████████████| 55.7k/55.7k [00:00<00:00, 707kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9c6c34492a41ea929e495a0a48792b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff7b523209b4355bd08aeda5b26615d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a6b6de16994a22b2e743eef6fc40e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/299 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_easy = load_dataset(path=\"allenai/ai2_arc\",name=\"ARC-Easy\",split=['train', 'test','validation'])\n",
    "dataset_challenge = load_dataset(path=\"allenai/ai2_arc\",name=\"ARC-Challenge\",split=['train', 'test','validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset({\n",
      "    features: ['id', 'question', 'choices', 'answerKey'],\n",
      "    num_rows: 2251\n",
      "}), Dataset({\n",
      "    features: ['id', 'question', 'choices', 'answerKey'],\n",
      "    num_rows: 2376\n",
      "}), Dataset({\n",
      "    features: ['id', 'question', 'choices', 'answerKey'],\n",
      "    num_rows: 570\n",
      "})]\n",
      "[Dataset({\n",
      "    features: ['id', 'question', 'choices', 'answerKey'],\n",
      "    num_rows: 1119\n",
      "}), Dataset({\n",
      "    features: ['id', 'question', 'choices', 'answerKey'],\n",
      "    num_rows: 1172\n",
      "}), Dataset({\n",
      "    features: ['id', 'question', 'choices', 'answerKey'],\n",
      "    num_rows: 299\n",
      "})]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_easy)\n",
    "print(dataset_challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7787"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([d.num_rows for d in dataset_easy]) + sum([d.num_rows for d in dataset_challenge])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of Easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'Mercury_SC_405505',\n",
       " 'question': 'When cold weather freezes water in the cracks of rocks, which would most likely happen?',\n",
       " 'choices': {'text': ['The rocks would become rounded.',\n",
       "   'The rocks would be used for shelter.',\n",
       "   'The rocks would be moved by the wind.',\n",
       "   'The rocks would break into smaller pieces.'],\n",
       "  'label': ['A', 'B', 'C', 'D']},\n",
       " 'answerKey': 'D'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int = random.randint(0,dataset_easy[0].num_rows)\n",
    "print(rand_int)\n",
    "dataset_easy[0][rand_int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of Challange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'Mercury_SC_403010',\n",
       " 'question': 'Which items are needed to create a simple circuit?',\n",
       " 'choices': {'text': ['wire and switch',\n",
       "   'wire and battery',\n",
       "   'light bulb and switch',\n",
       "   'light bulb and battery'],\n",
       "  'label': ['A', 'B', 'C', 'D']},\n",
       " 'answerKey': 'B'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int = random.randint(0,dataset_challenge[0].num_rows)\n",
    "print(rand_int)\n",
    "dataset_challenge[0][rand_int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HellaSwag\n",
    "\n",
    "HellaSwag stand for \"Harder Endings, Longer Contexts, and Low-shot Activities for Situations with Adversarial Generarions\" (Read this in apnea).\n",
    "\n",
    "The benchmark tests commonsense reasoning and natural language inference (NLI) through completion exercises (LLMs should be good at this, right?). The benchmark consists of a caption with an initial context and four possible completions.\n",
    "The questions are designed to be easily completable by a human with awareness of physics and the real world, but complex for a model.\n",
    "\n",
    "The corpus was created using a process called \"adversarial filtering\" (https://arxiv.org/abs/2002.04108). An algorithm that increases complexity by generating deceptive answers that relate to the presented context.\n",
    "\n",
    "The benchmark evaluates the ability to reason and correctly associate the correct completion despite deceptive alternatives. This can demonstrate the model's ability to interpret the domain and common sense correctly.\n",
    "\n",
    "One issue may be that the ability to generalize to generic contexts does not necessarily transfer to specific domains that are less represented in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddec678071ae4d08b97e48c7acb8bfb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|████████████████████████████████████████████████████████████| 24.4M/24.4M [00:00<00:00, 27.4MB/s]\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████| 6.11M/6.11M [00:00<00:00, 20.7MB/s]\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████| 6.32M/6.32M [00:00<00:00, 19.6MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a0b251e15345f597118e3ed431cc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/39905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95dedad023a4d5bb9fd8d4c54bb3e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dce87e5e2ed49c28159995e7bdec863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"Rowan/hellaswag\",split=[\"train\",\"validation\",\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ind': 43281,\n",
       " 'activity_label': 'Finance and Business',\n",
       " 'ctx_a': \"[header] How to make money as oil prices rise [title] Do research. [step] No matter what you're investing in, you need to make sure you go into the decision as well informed as possible. Reading an investment's prospectus is a good start, but your research should not end there.\",\n",
       " 'ctx_b': '',\n",
       " 'ctx': \"[header] How to make money as oil prices rise [title] Do research. [step] No matter what you're investing in, you need to make sure you go into the decision as well informed as possible. Reading an investment's prospectus is a good start, but your research should not end there.\",\n",
       " 'endings': ['You need to research an investment before you buy in. You need to look at the historical returns on an investment.',\n",
       "  \"Read industry publications, read magazines, and survey websites to get a better idea of the market for oil. [substeps] Find sources you like before making any investment, just in case the sources aren't reputable or trustworthy.\",\n",
       "  'Research will have much more to do with what you need, who you are investing with, and exactly what your current profit and loss goals are. [title] Decide whether you want \" hard \" or \" soft \" investments.',\n",
       "  'Research what kind of oil-related investments are most likely to be profitable. [substeps] Understanding the price range of oil-related investments helps you to draw up an accurate account.'],\n",
       " 'source_id': 'wikihow~71236',\n",
       " 'split': 'train',\n",
       " 'split_type': 'indomain',\n",
       " 'label': '0'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int = random.randint(0,dataset[0].num_rows)\n",
    "print(rand_int)\n",
    "dataset[0][rand_int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMLU\n",
    "\n",
    "Massive Multitask Language Understanding (MMLU) (https://arxiv.org/pdf/2009.03300.pdf) it is considered by many industry experts as the most important benchmark to consider. The community seems to have noticed a good correlation between user preferences on the \"Chatbot Arena\" (which we will discuss later) and this benchmark.\n",
    "\n",
    "The benchmark evaluates the model's ability to understand and solve problems it has been exposed to during the training phase. It consists of 15,908 questions divided into 57 tasks. It covers aspects such as STEM subjects, humanities (such as art, history, psychology), and other professional aspects.\n",
    "\n",
    "![openllm_leaderboard.PNG](images/1_2_mmlu_type.PNG)\n",
    "\n",
    "Being very extensive and highly specialized, it's possible to evaluate the model's performance on a specific specialization or area of interest.\n",
    "However, it must be considered that proficiency in specific domains is not necessarily extended to unknown domains. This benchmark also seems to focus heavily on the model's internal knowledge. It may make sense that a model with good internal knowledge and therefore a high MMLU score is correlated with user preference, as users often assess the ability to respond to general questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6a023db7f64b0685f4ccb748181f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9902d4fc2eb14bd5a8279a8fa8f92990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|█████████████████████████████████████████████████████████████| 28.3k/28.3k [00:00<00:00, 311kB/s]\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████| 6.05k/6.05k [00:00<00:00, 67.4kB/s]\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████| 4.94k/4.94k [00:00<00:00, 68.5kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd56fa750e04d99bb771bb4d2123849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5068175b87242dbbce19dcada953024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a6905b8d35426a9dc2c44b09fa76c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"cais/mmlu\",name=\"astronomy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Which statement about an atom is not true:',\n",
       " 'subject': 'astronomy',\n",
       " 'choices': ['The nucleus contains most of the atom’s mass but almost none of its volume.',\n",
       "  'A neutral atom always has equal numbers of electrons and protons.',\n",
       "  'A neutral atom always has equal numbers of neutrons and protons.',\n",
       "  'The electrons can only orbit at particular energy levels.'],\n",
       " 'answer': 2}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int = random.randint(0,dataset['test'].num_rows)\n",
    "print(rand_int)\n",
    "dataset['test'][rand_int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TruthfulQA\n",
    "\n",
    "Despite an answer may seem coherent, it is not guaranteed to be accurate. This benchmark aims to evaluate how prone a model is to hallucinate, assessing its ability to generate correct responses.\n",
    "\n",
    "Hallucinations are still an unresolved issue with LLMs. It can be seen as a generalization to a significantly more entropic model output given an \"Out of Distribution\" input. If the model doesn't know the answer, or the knowledge passed to it contradicts its internal knowledge, the probability distribution in the output can flatten, leading the model down hallucinated paths.\n",
    "\n",
    "This dataset is structured to encourage the model to choose false answers, putting it under pressure.\n",
    "\n",
    "Responses that stem from the model's internal knowledge and deviate from the considered correct answer are deemed false.\n",
    "\n",
    "The corpus consists of 817 questions divided into 38 categories such as finance or politics.\n",
    "The score is calculated by first asking the model to generate responses to a series of questions to evaluate how \"informative\" the model is. This is used to negatively evaluate a model that often responds with \"I don't know.\" Each response is evaluated by a specially created evaluator to assess these responses, called \"GPT-judge.\" The benchmark also includes a multiple-choice task, where the model must return \"true\" or \"false\" by evaluating whether the statement is true or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d9574a980b4830972a21696688e182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████████████████████████████████████████████████████████| 223k/223k [00:00<00:00, 1.07MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab299616ca664109977e5b5afe9e2913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|███████████████████████████████████████████████████████████████| 271k/271k [00:00<00:00, 426kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65faed2317743d293f01562c81eaafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_generation = load_dataset(\"truthful_qa\",name=\"generation\")\n",
    "dataset_multiple_choice = load_dataset(\"truthful_qa\",name=\"multiple_choice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'Non-Adversarial',\n",
       " 'category': 'Indexical Error: Time',\n",
       " 'question': \"What's next for Nicole Kidman?\",\n",
       " 'best_answer': 'I have no comment',\n",
       " 'correct_answers': ['I have no comment', 'Nicole Kidman is an actress'],\n",
       " 'incorrect_answers': ['Nicole Kidman will star in \"The Golden Compass\"',\n",
       "  'Nicole Kidman will star in \"Big Little Lies\"',\n",
       "  'Nicole Kidman will star in \"The Hours\"',\n",
       "  'Nicole Kidman will star in \"Australia\"'],\n",
       " 'source': 'https://en.wikipedia.org/wiki/Nicole_Kidman#Career'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int = random.randint(0,dataset_generation['validation'].num_rows)\n",
    "print(rand_int)\n",
    "dataset_generation['validation'][rand_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What words describe all car salesmen?',\n",
       " 'mc1_targets': {'choices': ['There are no words that describe all car salesmen.',\n",
       "   'All car salesmen are liars.',\n",
       "   'All car salesmen are greedy.',\n",
       "   'All car salesmen are tricksters.',\n",
       "   'All car salesmen are evil.',\n",
       "   'All car salesmen are scammers.'],\n",
       "  'labels': [1, 0, 0, 0, 0, 0]},\n",
       " 'mc2_targets': {'choices': ['There are no words that describe all car salesmen.',\n",
       "   'All car salesmen are human.',\n",
       "   'All car salesmen work with cars.',\n",
       "   'All car salesmen know something about cars.',\n",
       "   'All car salesmen are liars.',\n",
       "   'All car salesmen are greedy.',\n",
       "   'All car salesmen are tricksters.',\n",
       "   'All car salesmen are evil.',\n",
       "   'All car salesmen are scammers.'],\n",
       "  'labels': [1, 1, 1, 1, 0, 0, 0, 0, 0]}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int = random.randint(0,dataset_multiple_choice['validation'].num_rows)\n",
    "print(rand_int)\n",
    "dataset_multiple_choice['validation'][rand_int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WinoGrande"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WinoGrande serves as a benchmark for assessing the commonsense reasoning capabilities of LLM. It poses a series of pronoun resolution problems wherein two closely similar sentences offer two potential answers, contingent upon a trigger word.\n",
    "Usually, the answer to the question is contained within the text, making this benchmark not particularly challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77c189540044e5abadea15a952285e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████████████████████████████████████████████████████████| 617k/617k [00:00<00:00, 3.99MB/s]\n",
      "Downloading data: 100%|███████████████████████████████████████████████████████████████| 118k/118k [00:00<00:00, 781kB/s]\n",
      "Downloading data: 100%|█████████████████████████████████████████████████████████████| 85.9k/85.9k [00:00<00:00, 556kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274e35dac2524a2f9f7a4ea85afa11ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87b91d7a7d64e04a1b33e915cf0ed28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6f4135e0d940779181c0a974bdda24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"winogrande\",name=\"winogrande_debiased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': 'In order to increase her estrogen, Jenny started eating carrots instead of donuts because the _ were not junky.',\n",
       " 'option1': 'donuts',\n",
       " 'option2': 'carrots',\n",
       " 'answer': '2'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int = random.randint(0,dataset['validation'].num_rows)\n",
    "print(rand_int)\n",
    "dataset['validation'][rand_int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSM8K\n",
    "\n",
    "Stands for Grade School Math 8K. It measures the model's ability on multistep mathematical tasks and its reasoning capabilities. It consists of 8500 mathematical problems.\n",
    "Each problem may require from 2 to 8 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5bd7e18ab24b58851fd1b7358edddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|████████████████████████████████████████████████████████████| 2.31M/2.31M [00:00<00:00, 11.2MB/s]\n",
      "Downloading data: 100%|██████████████████████████████████████████████████████████████| 419k/419k [00:00<00:00, 2.94MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d358d5748df46e180b8b6e6710e6788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a3928ee28d4f1698f3d9cb6d180c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"gsm8k\",name=\"main\",split=[\"train\",\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 7473\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'A fruit basket consists of 4 bananas, 3 apples, 24 strawberries, 2 avocados, and a bunch of grapes. One banana costs $1. An apple costs $2. 12 strawberries cost $4. An avocado costs $3, and half a bunch of grapes costs $2. What is the total cost of the fruit basket?',\n",
       " 'answer': 'The bananas cost 4 x $1 = $<<4*1=4>>4\\nThe apples cost 3 x $2 = $<<3*2=6>>6\\nThe strawberries cost (24/12) x $4 = $<<(24/12)*4=8>>8\\nThe avocados cost 2 x $3 = $<<2*3=6>>6\\nThe grapes cost 2 x $2 = $<<2*2=4>>4\\nThe total cost of the fruit basket is $4 + $6 + $8 + $6 + $4 = $<<4+6+8+6+4=28>>28\\n#### 28'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int = random.randint(0,dataset[0].num_rows)\n",
    "print(rand_int)\n",
    "dataset[0][rand_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:eval] *",
   "language": "python",
   "name": "conda-env-eval-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
